<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Sijie Zhao, NJU">
<meta name="description" content="Sijie Zhao&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Sijie Zhao&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>

<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="760">
				<div id="toptitle">
					<h1>Sijie Zhao <font face="Arial">    赵思杰 </font></h1></div>
				<img src="./pic/others/email.png" height="22px">  sjzhao1996@gmail.com<br>
				<br>
				<img src="./pic/others/google_scholar_logo.png" height="22px">  <a href="https://scholar.google.com/citations?user=tZ3dS3MAAAAJ&hl">Google Sholar</a><br>
				<br>
				<img src="./pic/others/github_logo.png" height="22px">  <a href="https://github.com/sijeh">Github</a><br>
				<br>
				<!-- <img src="./pic/others/location.png" height="22px"> Shenzhen, China<br> -->

			</td>
			<td>
    <img src="./pic/sjzhao2.jpeg" border="0" width="120"><br>
   </td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2>Biography </h2>
<p>
	I am currently a researcher at  <a href="https://ailab.tencent.com/" target="_blank">Tencent AI Lab</a>, focusing on AIGC and multimodal foundation models. I got my B.Eng. and M.Eng. degree from <a href="https://www.nju.edu.cn/" target="_blank">Nanjing University</a>, <a href="https://ese.nju.edu.cn/" target="_blank">School of Electronic Science and Engineering</a>, advised by <a href="https://computationalperceptionlab.github.io" target="_blank">Prof. Tao Yue</a>
	<br>
</p>


<h2>News</h2>
<ul>
		<li>[09/2024] We release <a href = "https://stereocrafter.github.io/" target="_blank">StereoCrafter</a>, towards long and high-fidelity stereoscopic 3D generation from monocular videos.</li>
		<li>[09/2024] We release <a href = "https://depthcrafter.github.io/" target="_blank">DepthCrafter</a>, which is able to generate high-quality depth for open-world videos.</li>
		<li>[05/2024] We release <a href = "https://github.com/AILab-CVC/CV-VAE" target="_blank">CV-VAE</a>, a compatible video VAE for video generation.</li>
		<li>[04/2024] We release <a href = "https://github.com/AILab-CVC/SEED-X" target="_blank">SEED-X</a>, the latest in our SEED series, which unifies multi-granularity comprehension and generation.</li>
		<li>[10/2023] We release <a href = "https://github.com/AILab-CVC/SEED" target="_blank">SEED-LLaMA</a>, which is able to understand and generate images simultaneously</li>
</ul>




<h2> Publications</h2>
<table id="tbPublications" width="100%">
	<tbody>
					<tr>
	<td><center><img width="200" src="./pic/paper/stereocrafter.gif"></center></td>
	<td>
		<font size="2">StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos,
		<br>
		<i><b>Sijie Zhao*</b>, Wenbo Hu*, Xiaodong Cun*, Yong Zhang#, Xiaoyu Li#, Zhe Kong, Xiangjun Gao, Muyao Niu, Ying Shan </i>
		<br>
		Arxiv, 2024
		<br>
			[<a href='https://arxiv.org/abs/2409.07447' target="_blank"><b>paper </b></a>|<a href='https://stereocrafter.github.io/' target="_blank"><b> project</b></a>]
		</td>	
					<tr>

	<td><center><img width="200" src="./pic/paper/depthcrafter.gif"></center></td>
	<td>
		<font size="2">DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos,
		<br>
		<i>Wenbo Hu*#, Xiangjun Gao*, Xiaoyu Li*#, <b>Sijie Zhao</b>, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan </i>
		<br>
		Arxiv, 2024
		<br>
			[<a href='https://arxiv.org/abs/2409.02095' target="_blank"><b>paper </b></a>|<a href='https://depthcrafter.github.io/' target="_blank"><b> project </b></a>|<a href='https://github.com/Tencent/DepthCrafter' target="_blank"><b> code</b></a>]
		</td>	
					<tr>

	<td><center><img width="200" src="./pic/paper/cvvae.gif"></center></td>
	<td>
		<font size="2">CV-VAE: A Compatible Video VAE for Latent Generative Video Models,
		<br>
		<i><b>Sijie Zhao</b>, Yong Zhang#, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, Ying Shan </i>
		<br>
		NeurIPS, 2024
		<br>
			[<a href='https://arxiv.org/abs/2405.20279' target="_blank"><b>paper </b></a>|<a href='https://ailab-cvc.github.io/cvvae/index.html' target="_blank"><b> project </b></a>|<a href='https://github.com/AILab-CVC/CV-VAE' target="_blank"><b> code</b></a>]
		</td>	
					<tr>

	<td><center><img width="200" src="./pic/paper/seedx.png"></center></td>
	<td>
		<font size="2">SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation,
		<br>
		<i>Yuying Ge*, <b>Sijie Zhao*</b>, Jinguo Zhu*, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan, </i>
		<br>
		Arxiv, 2024
		<br>
			[<a href='https://arxiv.org/abs/2404.14396' target="_blank"><b>paper </b></a>|<a href='https://github.com/AILab-CVC/SEED-X' target="_blank"><b> code</b></a>]
		</td>	
					<tr>

	<td><center><img width="200" src="./pic/paper/unireplknet.jpg"></center></td>
	<td>
		<font size="2">UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio Video Point Cloud Time-Series and Image Recognition,
		<br>
		<i>Xiaohan Ding*, Yiyuan Zhang*, Yixiao Ge, <b>Sijie Zhao</b>, Lin Song, Xiangyu Yue, Ying Shan </i>
		<br>
		CVPR, 2024
		<br>
			[<a href='https://arxiv.org/abs/2311.15599' target="_blank"><b>paper </b></a>|<a href='https://github.com/AILab-CVC/UniRepLKNet' target="_blank"><b> code</b></a>]
		</td>	
					<tr>

	<td><center><img width="200" src="./pic/paper/seed_llama.jpg"></center></td>
	<td>
		<font size="2">Making LLaMA SEE and Draw with SEED Tokenizer,
		<br>
		<i>Yuying Ge*, <b>Sijie Zhao*</b>, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan </i>
		<br>
		ICLR, 2024
		<br>
			[<a href='https://arxiv.org/abs/2310.01218' target="_blank"><b>paper </b></a>|<a href='https://github.com/AILab-CVC/SEED/' target="_blank"><b> code </b></a>|<a href='https://ailab-cvc.github.io/seed/' target="_blank"><b> project</b></a>]
		</td>
					<tr>

	<td><center><img width="200" src="./pic/paper/gpt4tools.jpg"></center></td>
	<td>
		<font size="2">GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction,
		<br>
		<i>Rui Yang*, Lin Song*#, Yanwei Li, <b>Sijie Zhao</b>, Yixiao Ge, Xiu Li, Ying Shan </i>
		<br>
		NeurIPS, 2023
		<br>
			[<a href='https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf' target="_blank"><b>paper </b></a>|<a href='https://github.com/AILab-CVC/GPT4Tools' target="_blank"><b> code </b></a>|<a href='https://gpt4tools.github.io/' target="_blank"><b> project</b></a>]
		</td>
					<tr>

	<td><center><img width="200" src="./pic/paper/sticker820k.png"></center></td>
	<td>
		<font size="2">Sticker820K: Empowering Interactive Retrieval with Stickers,
		<br>
		<i><b>Sijie Zhao</b>, Yixiao Ge, Zhongang Qi, Lin Song, Xiaohan Ding, Zehua Xie, Ying Shan </i>
		<br>
		Arxiv, 2023
		<br>
			[<a href='https://arxiv.org/abs/2306.06870' target="_blank"><b>paper </b></a>]
		</td>

							<tr>

	<td><center><img width="200" src="./pic/paper/tof_imaging.png"></center></td>
	<td>
		<font size="2">Fisher Information Guidance for Learned Time-of-flight Imaging,
		<br>
		<i>Jiaqu Li, Tao Yue, <b>Sijie Zhao</b>, Xuemei Hu </i>
		<br>
		CVPR, 2022
		<br>
			[<a href='https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Fisher_Information_Guidance_for_Learned_Time-of-Flight_Imaging_CVPR_2022_paper.pdf' target="_blank"><b>paper </b></a>]
		</td>
							<tr>

	<td><center><img width="200" src="./pic/paper/damq.png"></center></td>
	<td>
		<font size="2">Distribution-aware Adaptive Multi-bit Quantization,
		<br>
		<i><b>Sijie Zhao</b>, Tao Yue, Xuemei Hu </i>
		<br>
		CVPR, 2021
		<br>
			[<a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Distribution-Aware_Adaptive_Multi-Bit_Quantization_CVPR_2021_paper.pdf' target="_blank"><b>paper </b></a>]
		</td>








</tbody></table>

	<h2><font> Education </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Master, School of Electronic Science and Engineering, Nanjing University, 2019 - 2022<br>
		  Bachelor, School of Electronic Science and Engineering, Nanjing University, 2015 - 2019<br>
	  </font> </p>
</ul>

	<h2><font> Experiences </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Researcher in Tencent AI Lab, 2022 - present<br>
		  Intern in Tencent AI Lab, 2021 - 2022 <br>
		  Intern in Ant Group, 2021. <br>
		  Intern in Kuaishou Inc. , 2021. <br>

	  </font> </p>
</ul>

<!-- <h2><font> Academic Activities </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Reviewer for CVPR, ICLR, ICML, NeurIPS, ECCV, ICCV, TPAMI, TNNLS, TMM, TVCJ<br>
		  Organizer of DeepFashion2 Challenge <a href='https://competitions.codalab.org/competitions/22966'>Clothes Landmark Detection</a>
			  and <a href='https://competitions.codalab.org/competitions/22967'>Clothes Retrieval</a> in 2019, 2020<br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative2020'>Third Workshop on Computer Vision for Fashion, Art and Design</a> in CVPR, 2020 <br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative/home?authuser=0'>Second Workshop on Computer Vision for Fashion, Art and Design</a> in ICCV, 2019 <br>
	  </font> </p>
</ul> -->




<!-- <p align=right>
	<a class="pull-right" href="#">
		<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1SUrJjJ6XinF5N0cMQTodfNbVKl2Bc7z1l6FEe_IT_I&cl=ffffff&w=a"></script></center>
	</a>
</p> -->



<!-- <p ><center>
	<div style="width:200px;height:200px;">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1SUrJjJ6XinF5N0cMQTodfNbVKl2Bc7z1l6FEe_IT_I&cl=ffffff&w=a"></script>
	</div>
</p> -->


<p><center><font>
	<div style="width:200px;">
	<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1SUrJjJ6XinF5N0cMQTodfNbVKl2Bc7z1l6FEe_IT_I&cl=ffffff&w=a"></script>-->
	<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=1SUrJjJ6XinF5N0cMQTodfNbVKl2Bc7z1l6FEe_IT_I"></script>
	</div>
        <br>&copy; Sijie Zhao | Last updated: Oct. 2024</font></center>
</p>

</div>
</body></html>
